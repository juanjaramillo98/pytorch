{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# work flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn  \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"minMaxScaled.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(692500, 54)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(df.columns)\n",
    "output_columns = [\"RENDIMIENTO_GLOBAL\"]\n",
    "input_columns = list(filter(lambda x: not 'RENDIMIENTO_GLOBAL' in x, columns))\n",
    "input_columns2 = list(filter(lambda x: not 'ESTU_PRGM_DEPARTAMENTO' in x, columns))\n",
    "\n",
    "salida = pd.get_dummies(df[\"RENDIMIENTO_GLOBAL\"],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_columns2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692495</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692496</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692497</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692498</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>692500 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1    2    3\n",
       "0       0.0  0.0  1.0  0.0\n",
       "1       1.0  0.0  0.0  0.0\n",
       "2       1.0  0.0  0.0  0.0\n",
       "3       0.0  0.0  0.0  1.0\n",
       "4       0.0  1.0  0.0  0.0\n",
       "...     ...  ...  ...  ...\n",
       "692495  0.0  0.0  1.0  0.0\n",
       "692496  1.0  0.0  0.0  0.0\n",
       "692497  0.0  1.0  0.0  0.0\n",
       "692498  1.0  0.0  0.0  0.0\n",
       "692499  0.0  0.0  0.0  1.0\n",
       "\n",
       "[692500 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(df[input_columns2].to_numpy()).type(torch.float).to(device)\n",
    "y = torch.from_numpy(df[output_columns].to_numpy()).type(torch.int64).to(device)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.01,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6925, 685575, torch.Size([685575, 23]), torch.Size([6925, 23]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test),len(X_train),X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6925, 685575, torch.Size([685575, 1]), torch.Size([6925, 1]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test),len(y_train),y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layer1): Linear(in_features=23, out_features=256, bias=True)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layer1 = nn.Linear(in_features=23,out_features=256)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 4)\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([2, 2, 2,  ..., 2, 0, 2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "logits = model(X_test)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer,print_status=True):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if print_status:\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), batch * batch_size + len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data_utils.TensorDataset(X_train, y_train.squeeze())\n",
    "test_data = data_utils.TensorDataset(X_test, y_test.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data,batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.390679  [   64/685575]\n",
      "loss: 1.391221  [ 6464/685575]\n",
      "loss: 1.384365  [12864/685575]\n",
      "loss: 1.385627  [19264/685575]\n",
      "loss: 1.379904  [25664/685575]\n",
      "loss: 1.386107  [32064/685575]\n",
      "loss: 1.385657  [38464/685575]\n",
      "loss: 1.383516  [44864/685575]\n",
      "loss: 1.382398  [51264/685575]\n",
      "loss: 1.383873  [57664/685575]\n",
      "loss: 1.372079  [64064/685575]\n",
      "loss: 1.374161  [70464/685575]\n",
      "loss: 1.376898  [76864/685575]\n",
      "loss: 1.372988  [83264/685575]\n",
      "loss: 1.373368  [89664/685575]\n",
      "loss: 1.371170  [96064/685575]\n",
      "loss: 1.368421  [102464/685575]\n",
      "loss: 1.374478  [108864/685575]\n",
      "loss: 1.366612  [115264/685575]\n",
      "loss: 1.375428  [121664/685575]\n",
      "loss: 1.358769  [128064/685575]\n",
      "loss: 1.365885  [134464/685575]\n",
      "loss: 1.370551  [140864/685575]\n",
      "loss: 1.361796  [147264/685575]\n",
      "loss: 1.364510  [153664/685575]\n",
      "loss: 1.364506  [160064/685575]\n",
      "loss: 1.371815  [166464/685575]\n",
      "loss: 1.358721  [172864/685575]\n",
      "loss: 1.347793  [179264/685575]\n",
      "loss: 1.348948  [185664/685575]\n",
      "loss: 1.352862  [192064/685575]\n",
      "loss: 1.350858  [198464/685575]\n",
      "loss: 1.353329  [204864/685575]\n",
      "loss: 1.352693  [211264/685575]\n",
      "loss: 1.346255  [217664/685575]\n",
      "loss: 1.343926  [224064/685575]\n",
      "loss: 1.334117  [230464/685575]\n",
      "loss: 1.349114  [236864/685575]\n",
      "loss: 1.336161  [243264/685575]\n",
      "loss: 1.331738  [249664/685575]\n",
      "loss: 1.355299  [256064/685575]\n",
      "loss: 1.324694  [262464/685575]\n",
      "loss: 1.327270  [268864/685575]\n",
      "loss: 1.334630  [275264/685575]\n",
      "loss: 1.329459  [281664/685575]\n",
      "loss: 1.339477  [288064/685575]\n",
      "loss: 1.344814  [294464/685575]\n",
      "loss: 1.316295  [300864/685575]\n",
      "loss: 1.325210  [307264/685575]\n",
      "loss: 1.318335  [313664/685575]\n",
      "loss: 1.347154  [320064/685575]\n",
      "loss: 1.322959  [326464/685575]\n",
      "loss: 1.336327  [332864/685575]\n",
      "loss: 1.324957  [339264/685575]\n",
      "loss: 1.324381  [345664/685575]\n",
      "loss: 1.321408  [352064/685575]\n",
      "loss: 1.321761  [358464/685575]\n",
      "loss: 1.335326  [364864/685575]\n",
      "loss: 1.304581  [371264/685575]\n",
      "loss: 1.313508  [377664/685575]\n",
      "loss: 1.331216  [384064/685575]\n",
      "loss: 1.322405  [390464/685575]\n",
      "loss: 1.337728  [396864/685575]\n",
      "loss: 1.291248  [403264/685575]\n",
      "loss: 1.312171  [409664/685575]\n",
      "loss: 1.313343  [416064/685575]\n",
      "loss: 1.315321  [422464/685575]\n",
      "loss: 1.310426  [428864/685575]\n",
      "loss: 1.303305  [435264/685575]\n",
      "loss: 1.306954  [441664/685575]\n",
      "loss: 1.307503  [448064/685575]\n",
      "loss: 1.295792  [454464/685575]\n",
      "loss: 1.309303  [460864/685575]\n",
      "loss: 1.292814  [467264/685575]\n",
      "loss: 1.299041  [473664/685575]\n",
      "loss: 1.290426  [480064/685575]\n",
      "loss: 1.283118  [486464/685575]\n",
      "loss: 1.280795  [492864/685575]\n",
      "loss: 1.279860  [499264/685575]\n",
      "loss: 1.299917  [505664/685575]\n",
      "loss: 1.294292  [512064/685575]\n",
      "loss: 1.272559  [518464/685575]\n",
      "loss: 1.274122  [524864/685575]\n",
      "loss: 1.303432  [531264/685575]\n",
      "loss: 1.284526  [537664/685575]\n",
      "loss: 1.264090  [544064/685575]\n",
      "loss: 1.247362  [550464/685575]\n",
      "loss: 1.249317  [556864/685575]\n",
      "loss: 1.262478  [563264/685575]\n",
      "loss: 1.284163  [569664/685575]\n",
      "loss: 1.271199  [576064/685575]\n",
      "loss: 1.266213  [582464/685575]\n",
      "loss: 1.255081  [588864/685575]\n",
      "loss: 1.245631  [595264/685575]\n",
      "loss: 1.276797  [601664/685575]\n",
      "loss: 1.266982  [608064/685575]\n",
      "loss: 1.254103  [614464/685575]\n",
      "loss: 1.303026  [620864/685575]\n",
      "loss: 1.239434  [627264/685575]\n",
      "loss: 1.255547  [633664/685575]\n",
      "loss: 1.252645  [640064/685575]\n",
      "loss: 1.271511  [646464/685575]\n",
      "loss: 1.250758  [652864/685575]\n",
      "loss: 1.249906  [659264/685575]\n",
      "loss: 1.283182  [665664/685575]\n",
      "loss: 1.244537  [672064/685575]\n",
      "loss: 1.242112  [678464/685575]\n",
      "loss: 1.288994  [684864/685575]\n",
      "Test Error: \n",
      " Accuracy: 50.6%, Avg loss: 1.241148 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.263755  [   64/685575]\n",
      "loss: 1.238334  [ 6464/685575]\n",
      "loss: 1.277017  [12864/685575]\n",
      "loss: 1.261162  [19264/685575]\n",
      "loss: 1.219078  [25664/685575]\n",
      "loss: 1.211818  [32064/685575]\n",
      "loss: 1.281084  [38464/685575]\n",
      "loss: 1.227787  [44864/685575]\n",
      "loss: 1.232104  [51264/685575]\n",
      "loss: 1.250986  [57664/685575]\n",
      "loss: 1.232642  [64064/685575]\n",
      "loss: 1.227361  [70464/685575]\n",
      "loss: 1.242169  [76864/685575]\n",
      "loss: 1.199909  [83264/685575]\n",
      "loss: 1.255474  [89664/685575]\n",
      "loss: 1.211526  [96064/685575]\n",
      "loss: 1.188331  [102464/685575]\n",
      "loss: 1.224196  [108864/685575]\n",
      "loss: 1.206661  [115264/685575]\n",
      "loss: 1.222190  [121664/685575]\n",
      "loss: 1.170167  [128064/685575]\n",
      "loss: 1.216276  [134464/685575]\n",
      "loss: 1.230392  [140864/685575]\n",
      "loss: 1.188090  [147264/685575]\n",
      "loss: 1.217776  [153664/685575]\n",
      "loss: 1.226552  [160064/685575]\n",
      "loss: 1.230942  [166464/685575]\n",
      "loss: 1.204499  [172864/685575]\n",
      "loss: 1.142603  [179264/685575]\n",
      "loss: 1.159492  [185664/685575]\n",
      "loss: 1.184654  [192064/685575]\n",
      "loss: 1.175778  [198464/685575]\n",
      "loss: 1.197314  [204864/685575]\n",
      "loss: 1.169925  [211264/685575]\n",
      "loss: 1.147119  [217664/685575]\n",
      "loss: 1.134659  [224064/685575]\n",
      "loss: 1.128868  [230464/685575]\n",
      "loss: 1.165734  [236864/685575]\n",
      "loss: 1.141569  [243264/685575]\n",
      "loss: 1.126521  [249664/685575]\n",
      "loss: 1.205796  [256064/685575]\n",
      "loss: 1.089301  [262464/685575]\n",
      "loss: 1.111116  [268864/685575]\n",
      "loss: 1.151829  [275264/685575]\n",
      "loss: 1.141439  [281664/685575]\n",
      "loss: 1.169851  [288064/685575]\n",
      "loss: 1.180119  [294464/685575]\n",
      "loss: 1.093075  [300864/685575]\n",
      "loss: 1.125986  [307264/685575]\n",
      "loss: 1.107046  [313664/685575]\n",
      "loss: 1.176992  [320064/685575]\n",
      "loss: 1.116129  [326464/685575]\n",
      "loss: 1.149169  [332864/685575]\n",
      "loss: 1.124094  [339264/685575]\n",
      "loss: 1.126853  [345664/685575]\n",
      "loss: 1.114638  [352064/685575]\n",
      "loss: 1.104265  [358464/685575]\n",
      "loss: 1.155893  [364864/685575]\n",
      "loss: 1.078102  [371264/685575]\n",
      "loss: 1.093933  [377664/685575]\n",
      "loss: 1.122165  [384064/685575]\n",
      "loss: 1.147272  [390464/685575]\n",
      "loss: 1.131958  [396864/685575]\n",
      "loss: 1.060495  [403264/685575]\n",
      "loss: 1.084451  [409664/685575]\n",
      "loss: 1.099406  [416064/685575]\n",
      "loss: 1.097357  [422464/685575]\n",
      "loss: 1.085105  [428864/685575]\n",
      "loss: 1.093868  [435264/685575]\n",
      "loss: 1.076548  [441664/685575]\n",
      "loss: 1.088910  [448064/685575]\n",
      "loss: 1.070373  [454464/685575]\n",
      "loss: 1.088685  [460864/685575]\n",
      "loss: 1.038225  [467264/685575]\n",
      "loss: 1.073114  [473664/685575]\n",
      "loss: 1.056338  [480064/685575]\n",
      "loss: 1.027359  [486464/685575]\n",
      "loss: 1.052933  [492864/685575]\n",
      "loss: 1.017533  [499264/685575]\n",
      "loss: 1.076785  [505664/685575]\n",
      "loss: 1.062475  [512064/685575]\n",
      "loss: 1.030170  [518464/685575]\n",
      "loss: 1.027529  [524864/685575]\n",
      "loss: 1.079895  [531264/685575]\n",
      "loss: 1.059865  [537664/685575]\n",
      "loss: 0.996466  [544064/685575]\n",
      "loss: 0.978861  [550464/685575]\n",
      "loss: 0.979356  [556864/685575]\n",
      "loss: 0.973055  [563264/685575]\n",
      "loss: 1.042642  [569664/685575]\n",
      "loss: 1.031158  [576064/685575]\n",
      "loss: 0.983981  [582464/685575]\n",
      "loss: 0.970953  [588864/685575]\n",
      "loss: 0.962922  [595264/685575]\n",
      "loss: 1.041934  [601664/685575]\n",
      "loss: 1.022372  [608064/685575]\n",
      "loss: 0.983550  [614464/685575]\n",
      "loss: 1.031943  [620864/685575]\n",
      "loss: 0.972568  [627264/685575]\n",
      "loss: 1.005146  [633664/685575]\n",
      "loss: 0.963071  [640064/685575]\n",
      "loss: 1.020480  [646464/685575]\n",
      "loss: 0.968761  [652864/685575]\n",
      "loss: 0.970425  [659264/685575]\n",
      "loss: 1.043118  [665664/685575]\n",
      "loss: 0.952443  [672064/685575]\n",
      "loss: 0.983124  [678464/685575]\n",
      "loss: 1.030421  [684864/685575]\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.968986 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.020632  [   64/685575]\n",
      "loss: 0.951344  [ 6464/685575]\n",
      "loss: 1.016286  [12864/685575]\n",
      "loss: 0.944691  [19264/685575]\n",
      "loss: 0.918410  [25664/685575]\n",
      "loss: 0.920460  [32064/685575]\n",
      "loss: 1.056519  [38464/685575]\n",
      "loss: 0.935296  [44864/685575]\n",
      "loss: 0.946792  [51264/685575]\n",
      "loss: 0.986953  [57664/685575]\n",
      "loss: 0.942863  [64064/685575]\n",
      "loss: 0.941827  [70464/685575]\n",
      "loss: 0.962353  [76864/685575]\n",
      "loss: 0.911918  [83264/685575]\n",
      "loss: 0.996223  [89664/685575]\n",
      "loss: 0.932431  [96064/685575]\n",
      "loss: 0.877916  [102464/685575]\n",
      "loss: 0.965664  [108864/685575]\n",
      "loss: 0.950156  [115264/685575]\n",
      "loss: 0.947068  [121664/685575]\n",
      "loss: 0.873109  [128064/685575]\n",
      "loss: 0.943636  [134464/685575]\n",
      "loss: 0.929415  [140864/685575]\n",
      "loss: 0.904054  [147264/685575]\n",
      "loss: 0.932841  [153664/685575]\n",
      "loss: 0.953752  [160064/685575]\n",
      "loss: 0.937084  [166464/685575]\n",
      "loss: 0.915803  [172864/685575]\n",
      "loss: 0.843114  [179264/685575]\n",
      "loss: 0.868565  [185664/685575]\n",
      "loss: 0.887924  [192064/685575]\n",
      "loss: 0.877206  [198464/685575]\n",
      "loss: 0.926451  [204864/685575]\n",
      "loss: 0.843850  [211264/685575]\n",
      "loss: 0.818305  [217664/685575]\n",
      "loss: 0.804610  [224064/685575]\n",
      "loss: 0.835076  [230464/685575]\n",
      "loss: 0.840972  [236864/685575]\n",
      "loss: 0.828120  [243264/685575]\n",
      "loss: 0.835578  [249664/685575]\n",
      "loss: 0.915386  [256064/685575]\n",
      "loss: 0.771865  [262464/685575]\n",
      "loss: 0.801594  [268864/685575]\n",
      "loss: 0.859681  [275264/685575]\n",
      "loss: 0.869048  [281664/685575]\n",
      "loss: 0.881765  [288064/685575]\n",
      "loss: 0.879898  [294464/685575]\n",
      "loss: 0.786718  [300864/685575]\n",
      "loss: 0.831421  [307264/685575]\n",
      "loss: 0.803522  [313664/685575]\n",
      "loss: 0.857734  [320064/685575]\n",
      "loss: 0.785250  [326464/685575]\n",
      "loss: 0.839890  [332864/685575]\n",
      "loss: 0.810232  [339264/685575]\n",
      "loss: 0.822821  [345664/685575]\n",
      "loss: 0.806877  [352064/685575]\n",
      "loss: 0.771513  [358464/685575]\n",
      "loss: 0.852730  [364864/685575]\n",
      "loss: 0.769495  [371264/685575]\n",
      "loss: 0.781285  [377664/685575]\n",
      "loss: 0.774970  [384064/685575]\n",
      "loss: 0.873406  [390464/685575]\n",
      "loss: 0.789616  [396864/685575]\n",
      "loss: 0.769789  [403264/685575]\n",
      "loss: 0.756231  [409664/685575]\n",
      "loss: 0.782157  [416064/685575]\n",
      "loss: 0.773987  [422464/685575]\n",
      "loss: 0.768783  [428864/685575]\n",
      "loss: 0.800409  [435264/685575]\n",
      "loss: 0.754480  [441664/685575]\n",
      "loss: 0.777702  [448064/685575]\n",
      "loss: 0.769799  [454464/685575]\n",
      "loss: 0.780401  [460864/685575]\n",
      "loss: 0.706460  [467264/685575]\n",
      "loss: 0.763128  [473664/685575]\n",
      "loss: 0.749199  [480064/685575]\n",
      "loss: 0.707698  [486464/685575]\n",
      "loss: 0.770657  [492864/685575]\n",
      "loss: 0.692142  [499264/685575]\n",
      "loss: 0.771989  [505664/685575]\n",
      "loss: 0.748834  [512064/685575]\n",
      "loss: 0.740489  [518464/685575]\n",
      "loss: 0.728069  [524864/685575]\n",
      "loss: 0.769204  [531264/685575]\n",
      "loss: 0.760518  [537664/685575]\n",
      "loss: 0.681940  [544064/685575]\n",
      "loss: 0.690985  [550464/685575]\n",
      "loss: 0.688414  [556864/685575]\n",
      "loss: 0.649425  [563264/685575]\n",
      "loss: 0.735267  [569664/685575]\n",
      "loss: 0.735488  [576064/685575]\n",
      "loss: 0.656415  [582464/685575]\n",
      "loss: 0.655193  [588864/685575]\n",
      "loss: 0.657318  [595264/685575]\n",
      "loss: 0.742393  [601664/685575]\n",
      "loss: 0.727758  [608064/685575]\n",
      "loss: 0.681237  [614464/685575]\n",
      "loss: 0.686214  [620864/685575]\n",
      "loss: 0.683209  [627264/685575]\n",
      "loss: 0.717727  [633664/685575]\n",
      "loss: 0.644864  [640064/685575]\n",
      "loss: 0.717629  [646464/685575]\n",
      "loss: 0.660146  [652864/685575]\n",
      "loss: 0.658359  [659264/685575]\n",
      "loss: 0.738793  [665664/685575]\n",
      "loss: 0.641554  [672064/685575]\n",
      "loss: 0.693876  [678464/685575]\n",
      "loss: 0.712971  [684864/685575]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.674408 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.728338  [   64/685575]\n",
      "loss: 0.653853  [ 6464/685575]\n",
      "loss: 0.708240  [12864/685575]\n",
      "loss: 0.606495  [19264/685575]\n",
      "loss: 0.619246  [25664/685575]\n",
      "loss: 0.632874  [32064/685575]\n",
      "loss: 0.763222  [38464/685575]\n",
      "loss: 0.638223  [44864/685575]\n",
      "loss: 0.654883  [51264/685575]\n",
      "loss: 0.694414  [57664/685575]\n",
      "loss: 0.643606  [64064/685575]\n",
      "loss: 0.650944  [70464/685575]\n",
      "loss: 0.663026  [76864/685575]\n",
      "loss: 0.633332  [83264/685575]\n",
      "loss: 0.700250  [89664/685575]\n",
      "loss: 0.651082  [96064/685575]\n",
      "loss: 0.592841  [102464/685575]\n",
      "loss: 0.682827  [108864/685575]\n",
      "loss: 0.679733  [115264/685575]\n",
      "loss: 0.654068  [121664/685575]\n",
      "loss: 0.604319  [128064/685575]\n",
      "loss: 0.664411  [134464/685575]\n",
      "loss: 0.623546  [140864/685575]\n",
      "loss: 0.631566  [147264/685575]\n",
      "loss: 0.640531  [153664/685575]\n",
      "loss: 0.659920  [160064/685575]\n",
      "loss: 0.629812  [166464/685575]\n",
      "loss: 0.635317  [172864/685575]\n",
      "loss: 0.579422  [179264/685575]\n",
      "loss: 0.607599  [185664/685575]\n",
      "loss: 0.611956  [192064/685575]\n",
      "loss: 0.598444  [198464/685575]\n",
      "loss: 0.651413  [204864/685575]\n",
      "loss: 0.557885  [211264/685575]\n",
      "loss: 0.541979  [217664/685575]\n",
      "loss: 0.544239  [224064/685575]\n",
      "loss: 0.583070  [230464/685575]\n",
      "loss: 0.563025  [236864/685575]\n",
      "loss: 0.558221  [243264/685575]\n",
      "loss: 0.581080  [249664/685575]\n",
      "loss: 0.629770  [256064/685575]\n",
      "loss: 0.518745  [262464/685575]\n",
      "loss: 0.552000  [268864/685575]\n",
      "loss: 0.593152  [275264/685575]\n",
      "loss: 0.618155  [281664/685575]\n",
      "loss: 0.603164  [288064/685575]\n",
      "loss: 0.606338  [294464/685575]\n",
      "loss: 0.543765  [300864/685575]\n",
      "loss: 0.573487  [307264/685575]\n",
      "loss: 0.552287  [313664/685575]\n",
      "loss: 0.574515  [320064/685575]\n",
      "loss: 0.524938  [326464/685575]\n",
      "loss: 0.575884  [332864/685575]\n",
      "loss: 0.541456  [339264/685575]\n",
      "loss: 0.567471  [345664/685575]\n",
      "loss: 0.549260  [352064/685575]\n",
      "loss: 0.509844  [358464/685575]\n",
      "loss: 0.574390  [364864/685575]\n",
      "loss: 0.530035  [371264/685575]\n",
      "loss: 0.528617  [377664/685575]\n",
      "loss: 0.501072  [384064/685575]\n",
      "loss: 0.602359  [390464/685575]\n",
      "loss: 0.507852  [396864/685575]\n",
      "loss: 0.530076  [403264/685575]\n",
      "loss: 0.507644  [409664/685575]\n",
      "loss: 0.534161  [416064/685575]\n",
      "loss: 0.514075  [422464/685575]\n",
      "loss: 0.521126  [428864/685575]\n",
      "loss: 0.562940  [435264/685575]\n",
      "loss: 0.496078  [441664/685575]\n",
      "loss: 0.525397  [448064/685575]\n",
      "loss: 0.520610  [454464/685575]\n",
      "loss: 0.532528  [460864/685575]\n",
      "loss: 0.470144  [467264/685575]\n",
      "loss: 0.503877  [473664/685575]\n",
      "loss: 0.507773  [480064/685575]\n",
      "loss: 0.467887  [486464/685575]\n",
      "loss: 0.531626  [492864/685575]\n",
      "loss: 0.457832  [499264/685575]\n",
      "loss: 0.523789  [505664/685575]\n",
      "loss: 0.494036  [512064/685575]\n",
      "loss: 0.504186  [518464/685575]\n",
      "loss: 0.489196  [524864/685575]\n",
      "loss: 0.514898  [531264/685575]\n",
      "loss: 0.518067  [537664/685575]\n",
      "loss: 0.451953  [544064/685575]\n",
      "loss: 0.471547  [550464/685575]\n",
      "loss: 0.483697  [556864/685575]\n",
      "loss: 0.422023  [563264/685575]\n",
      "loss: 0.489413  [569664/685575]\n",
      "loss: 0.501843  [576064/685575]\n",
      "loss: 0.430081  [582464/685575]\n",
      "loss: 0.431407  [588864/685575]\n",
      "loss: 0.445091  [595264/685575]\n",
      "loss: 0.502346  [601664/685575]\n",
      "loss: 0.492603  [608064/685575]\n",
      "loss: 0.438415  [614464/685575]\n",
      "loss: 0.434936  [620864/685575]\n",
      "loss: 0.454035  [627264/685575]\n",
      "loss: 0.486202  [633664/685575]\n",
      "loss: 0.413639  [640064/685575]\n",
      "loss: 0.478022  [646464/685575]\n",
      "loss: 0.439968  [652864/685575]\n",
      "loss: 0.423551  [659264/685575]\n",
      "loss: 0.482607  [665664/685575]\n",
      "loss: 0.414288  [672064/685575]\n",
      "loss: 0.460235  [678464/685575]\n",
      "loss: 0.464413  [684864/685575]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.449515 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.484735  [   64/685575]\n",
      "loss: 0.433484  [ 6464/685575]\n",
      "loss: 0.471632  [12864/685575]\n",
      "loss: 0.370667  [19264/685575]\n",
      "loss: 0.410840  [25664/685575]\n",
      "loss: 0.419366  [32064/685575]\n",
      "loss: 0.505411  [38464/685575]\n",
      "loss: 0.414953  [44864/685575]\n",
      "loss: 0.435912  [51264/685575]\n",
      "loss: 0.461919  [57664/685575]\n",
      "loss: 0.425946  [64064/685575]\n",
      "loss: 0.431489  [70464/685575]\n",
      "loss: 0.432855  [76864/685575]\n",
      "loss: 0.423931  [83264/685575]\n",
      "loss: 0.465946  [89664/685575]\n",
      "loss: 0.433187  [96064/685575]\n",
      "loss: 0.387556  [102464/685575]\n",
      "loss: 0.449178  [108864/685575]\n",
      "loss: 0.458757  [115264/685575]\n",
      "loss: 0.415460  [121664/685575]\n",
      "loss: 0.403843  [128064/685575]\n",
      "loss: 0.445204  [134464/685575]\n",
      "loss: 0.393400  [140864/685575]\n",
      "loss: 0.419332  [147264/685575]\n",
      "loss: 0.415337  [153664/685575]\n",
      "loss: 0.427371  [160064/685575]\n",
      "loss: 0.390734  [166464/685575]\n",
      "loss: 0.422473  [172864/685575]\n",
      "loss: 0.379519  [179264/685575]\n",
      "loss: 0.408270  [185664/685575]\n",
      "loss: 0.404695  [192064/685575]\n",
      "loss: 0.387413  [198464/685575]\n",
      "loss: 0.433387  [204864/685575]\n",
      "loss: 0.351963  [211264/685575]\n",
      "loss: 0.345898  [217664/685575]\n",
      "loss: 0.365228  [224064/685575]\n",
      "loss: 0.392282  [230464/685575]\n",
      "loss: 0.362761  [236864/685575]\n",
      "loss: 0.359265  [243264/685575]\n",
      "loss: 0.386832  [249664/685575]\n",
      "loss: 0.409375  [256064/685575]\n",
      "loss: 0.332438  [262464/685575]\n",
      "loss: 0.366730  [268864/685575]\n",
      "loss: 0.386316  [275264/685575]\n",
      "loss: 0.418291  [281664/685575]\n",
      "loss: 0.386871  [288064/685575]\n",
      "loss: 0.401315  [294464/685575]\n",
      "loss: 0.361262  [300864/685575]\n",
      "loss: 0.372910  [307264/685575]\n",
      "loss: 0.362802  [313664/685575]\n",
      "loss: 0.367676  [320064/685575]\n",
      "loss: 0.338632  [326464/685575]\n",
      "loss: 0.381045  [332864/685575]\n",
      "loss: 0.343747  [339264/685575]\n",
      "loss: 0.376046  [345664/685575]\n",
      "loss: 0.358264  [352064/685575]\n",
      "loss: 0.321571  [358464/685575]\n",
      "loss: 0.363629  [364864/685575]\n",
      "loss: 0.351425  [371264/685575]\n",
      "loss: 0.339438  [377664/685575]\n",
      "loss: 0.307884  [384064/685575]\n",
      "loss: 0.389269  [390464/685575]\n",
      "loss: 0.309016  [396864/685575]\n",
      "loss: 0.345895  [403264/685575]\n",
      "loss: 0.330443  [409664/685575]\n",
      "loss: 0.355054  [416064/685575]\n",
      "loss: 0.326513  [422464/685575]\n",
      "loss: 0.340831  [428864/685575]\n",
      "loss: 0.382222  [435264/685575]\n",
      "loss: 0.312740  [441664/685575]\n",
      "loss: 0.341402  [448064/685575]\n",
      "loss: 0.334165  [454464/685575]\n",
      "loss: 0.348179  [460864/685575]\n",
      "loss: 0.302479  [467264/685575]\n",
      "loss: 0.315723  [473664/685575]\n",
      "loss: 0.328755  [480064/685575]\n",
      "loss: 0.293481  [486464/685575]\n",
      "loss: 0.351954  [492864/685575]\n",
      "loss: 0.291316  [499264/685575]\n",
      "loss: 0.341459  [505664/685575]\n",
      "loss: 0.308489  [512064/685575]\n",
      "loss: 0.327055  [518464/685575]\n",
      "loss: 0.313513  [524864/685575]\n",
      "loss: 0.330448  [531264/685575]\n",
      "loss: 0.335625  [537664/685575]\n",
      "loss: 0.284163  [544064/685575]\n",
      "loss: 0.309457  [550464/685575]\n",
      "loss: 0.329158  [556864/685575]\n",
      "loss: 0.263306  [563264/685575]\n",
      "loss: 0.308326  [569664/685575]\n",
      "loss: 0.327074  [576064/685575]\n",
      "loss: 0.267642  [582464/685575]\n",
      "loss: 0.273607  [588864/685575]\n",
      "loss: 0.288146  [595264/685575]\n",
      "loss: 0.319908  [601664/685575]\n",
      "loss: 0.317128  [608064/685575]\n",
      "loss: 0.268774  [614464/685575]\n",
      "loss: 0.266563  [620864/685575]\n",
      "loss: 0.284033  [627264/685575]\n",
      "loss: 0.313367  [633664/685575]\n",
      "loss: 0.253907  [640064/685575]\n",
      "loss: 0.303140  [646464/685575]\n",
      "loss: 0.278218  [652864/685575]\n",
      "loss: 0.260678  [659264/685575]\n",
      "loss: 0.302032  [665664/685575]\n",
      "loss: 0.257722  [672064/685575]\n",
      "loss: 0.289074  [678464/685575]\n",
      "loss: 0.290669  [684864/685575]\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.286815 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer,False)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"modelo99\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model2 = NeuralNetwork2().to(\"cuda\")\n",
    "#model2.load_state_dict(torch.load(\"modelo_0\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 41.3%, Avg loss: 1.243838 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_dataloader, model2, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
